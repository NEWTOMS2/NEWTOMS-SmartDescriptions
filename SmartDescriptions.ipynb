{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, tune, and deploy a custom ML model using SmartDescriptions Algorithm from AWS Marketplace \n",
    "\n",
    "\n",
    "SmartDescriptions is a data-to-text solution that allows you to generate text from structured data. With the SmartDescriptions solution you can save time by generating thousands of comprehensible texts automatically.\n",
    "\n",
    "Large companies whose business model is the sale are always faced with the challenge of describing their hundreds of products and services. With this solution you can create products descriptions at the push of a button.\n",
    "\n",
    "You can finetune the SmartDescriptions solution to start generating texts according to your business domain. \n",
    "\n",
    "This sample notebook shows you how to train a custom ML model using [SmartDescriptions](https://aws.amazon.com/marketplace/management/ml-products/a2b91337-b40d-4eb3-a915-53c42f01ccea?) from AWS Marketplace.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. Some hands-on experience using [Amazon SageMaker](https://aws.amazon.com/sagemaker/).\n",
    "1. To use this algorithm successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "    2. or your AWS account has a subscription to [SmartDescriptions](https://aws.amazon.com/marketplace/management/ml-products/a2b91337-b40d-4eb3-a915-53c42f01ccea?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "1. [Subscribe to the algorithm](#1.-Subscribe-to-the-algorithm)\n",
    "1. [Prepare dataset](#2.-Prepare-dataset)\n",
    "\t1. [Dataset format expected by the algorithm](#A.-Dataset-format-expected-by-the-algorithm)\n",
    "\t1. [Configure and visualize train and test dataset](#B.-Configure-and-visualize-train-and-test-dataset)\n",
    "\t1. [Upload datasets to Amazon S3](#C.-Upload-datasets-to-Amazon-S3)\n",
    "1. [Train a machine learning model](#3:-Train-a-machine-learning-model)\n",
    "\t1. [Set up environment](#3.1-Set-up-environment)\n",
    "\t1. [Train a model](#3.2-Train-a-model)\n",
    "1. [Deploy model and verify results](#4:-Deploy-model-and-verify-results)\n",
    "    1. [Deploy trained model](#A.-Deploy-trained-model)\n",
    "    1. [Create input payload](#B.-Create-input-payload)\n",
    "    1. [Perform real-time inference](#C.-Perform-real-time-inference)\n",
    "    1. [Visualize output](#D.-Visualize-output)\n",
    "    1. [Delete the endpoint](#E.-Delete-the-endpoint)\n",
    "1. [Perform Batch inference](#5.-Perform-Batch-inference)\n",
    "1. [Clean-up](#6.-Clean-up)\n",
    "\t1. [Delete the model](#A.-Delete-the-model)\n",
    "\n",
    "\n",
    "## Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Subscribe to the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To subscribe to the algorithm:\n",
    "1. Open the algorithm listing page [SmartDescriptions](https://aws.amazon.com/marketplace/management/ml-products/a2b91337-b40d-4eb3-a915-53c42f01ccea?)\n",
    "1. On the AWS Marketplace listing,  click on **Continue to subscribe** button.\n",
    "1. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you agree with EULA, pricing, and support terms. \n",
    "1. Once you click on **Continue to configuration button** and then choose a **region**, you will see a **Product Arn**. This is the algorithm ARN that you need to specify while training a custom ML model. Copy the ARN corresponding to your region and specify the same in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_arn = \"<AlgorithmARN>\" # Replace this with your algorithm ARN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import AlgorithmEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Dataset format expected by the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm requires a **JSON line** file, each line will contain an object with the attributes \"data\" and \"response\" in this respective order. The attribute \"data\" corresponds to your structured data separated by \";\", and the attribute \"response\" corresponds to your desire response from the structured data specified.\n",
    "\n",
    "The JSON line file should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': '<name> name = [ Ala Moana Hotel ] ; <address> address = [ 410 Atkinson Drive, Honolulu, HI 96814, USA ] ; <feelsHotel> feelsHotel = [ casual ] ; <hasConventionCenter> hasConventionCenter = [ yes ] ; <hasOnsiteCafe> hasOnsiteCafe = [ yes ] ; <hasCoffeeInRooms> hasCoffeeInRooms = [ yes ] ; <hasMinifridgeInRooms> hasMinifridgeInRooms = [ yes ] ; <hasBalconyInRooms> hasBalconyInRooms = [ yes ] ; <hasMicrowaveInRooms> hasMicrowaveInRooms = [ yes ]',\n",
       " 'response': \"It would be good idea to consider Ala Moana Hotel 410 Atkinson Drive, Honolulu, HI 96814, USA. It's casual, with an onsite cafe and a convention center. This hotel has microwave, mini fridge , coffee in rooms and balcony.\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"data\": \"<name> name = [ 1 Hotel South Beach ] ; <address> address = [ 2341 Collins Ave, Miami Beach, FL 33139, USA ] ; <feelsHotel> feelsHotel = [ luxury ] ; <hasBarOnsite> hasBarOnsite = [ yes ] ; <hasDeskInRooms> hasDeskInRooms = [ yes ] ; <hasBalconyInRooms> hasBalconyInRooms = [ yes ] ; <hasRoomsUpgraded> hasRoomsUpgraded = [ yes ] ; <hasKitchenInRoom> hasKitchenInRoom = [ yes ]\", \"response\": \"A good choice is 1 Hotel South Beach in Miami Beach. It's luxurious, with a bar onsite. The upgraded rooms are full featured, including a kitchen and a desk for work. Each room also has a balcony.\"}\n",
    "{\"data\": \"<name> name = [ Ala Moana Hotel ] ; <address> address = [ 410 Atkinson Drive, Honolulu, HI 96814, USA ] ; <feelsHotel> feelsHotel = [ casual ] ; <hasConventionCenter> hasConventionCenter = [ yes ] ; <hasOnsiteCafe> hasOnsiteCafe = [ yes ] ; <hasCoffeeInRooms> hasCoffeeInRooms = [ yes ] ; <hasMinifridgeInRooms> hasMinifridgeInRooms = [ yes ] ; <hasBalconyInRooms> hasBalconyInRooms = [ yes ] ; <hasMicrowaveInRooms> hasMicrowaveInRooms = [ yes ]\", \"response\": \"It would be good idea to consider Ala Moana Hotel 410 Atkinson Drive, Honolulu, HI 96814, USA. It's casual, with an onsite cafe and a convention center. This hotel has microwave, mini fridge , coffee in rooms and balcony.\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You should name your data file with extension .json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also find more information about dataset format in **Usage Information** section of [SmartDescriptions](https://aws.amazon.com/marketplace/management/ml-products/a2b91337-b40d-4eb3-a915-53c42f01ccea?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Configure and visualize train and test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must upload the training dataset into data/train directory and update the `training_file_name` parameter value in following cell. **If you intend to download it at run-time, add relevant code in following cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_name = \"<FileName.json>\"\n",
    "training_dataset = \"./data/train/{}\".format(training_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(training_dataset) as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'data': '<name> name = [ 1 Hotel South Beach ] ; <address> address = [ 2341 Collins Ave, Miami Beach, FL 33139, USA ] ; <feelsHotel> feelsHotel = [ luxury ] ; <hasBarOnsite> hasBarOnsite = [ yes ] ; <hasDeskInRooms> hasDeskInRooms = [ yes ] ; <hasBalconyInRooms> hasBalconyInRooms = [ yes ] ; <hasRoomsUpgraded> hasRoomsUpgraded = [ yes ] ; <hasKitchenInRoom> hasKitchenInRoom = [ yes ]',\n",
       "  'response': \"A good choice is 1 Hotel South Beach in Miami Beach. It's luxurious, with a bar onsite. The upgraded rooms are full featured, including a kitchen and a desk for work. Each room also has a balcony.\"},\n",
       " {'data': '<name> name = [ 1 Hotel South Beach ] ; <address> address = [ 2341 Collins Ave, Miami Beach, FL 33139, USA ] ; <feelsHotel> feelsHotel = [ luxury ] ; <hasBarOnsite> hasBarOnsite = [ yes ] ; <hasDeskInRooms> hasDeskInRooms = [ yes ] ; <hasBalconyInRooms> hasBalconyInRooms = [ yes ] ; <hasRoomsUpgraded> hasRoomsUpgraded = [ yes ] ; <hasKitchenInRoom> hasKitchenInRoom = [ yes ]',\n",
       "  'response': \"I think that 1 Hotel South Beach will meet your needs. It's a luxury hotel with a bar. It features upgraded rooms with a fully equipped kitchen. Each room features a balcony with exquisite beach views. All rooms offer a small office area with a mahogany wood desk and comfortable leather chair.\"},\n",
       " {'data': '<name> name = [ 1 Hotel South Beach ] ; <address> address = [ 2341 Collins Ave, Miami Beach, FL 33139, USA ] ; <feelsHotel> feelsHotel = [ luxury ] ; <hasBarOnsite> hasBarOnsite = [ yes ] ; <hasDeskInRooms> hasDeskInRooms = [ yes ] ; <hasBalconyInRooms> hasBalconyInRooms = [ yes ] ; <hasRoomsUpgraded> hasRoomsUpgraded = [ yes ] ; <hasKitchenInRoom> hasKitchenInRoom = [ yes ]',\n",
       "  'response': \"The 1 Hotel South Beach doesn't mess around. When you come to stay here you won't want to leave. It has luxurious onsite bar. Each upgraded room features a sunny balcony and personal kitchen. You also can expect to find a lovely writing desk for your all correspondence needs.\"}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show the training data\n",
    "data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Upload datasets to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sage.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = sagemaker_session.upload_data(\n",
    "    training_dataset, bucket=bucket, key_prefix=\"smart-descriptions/train\"\n",
    ")\n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Train a machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that dataset is available in an accessible Amazon S3 bucket, we are ready to train a machine learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_location = \"s3://{}/smart-descriptions/{}\".format(\n",
    "    bucket, \"output\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also find more information about dataset format in **Hyperparameters** section of [SmartDescriptions](https://aws.amazon.com/marketplace/management/ml-products/a2b91337-b40d-4eb3-a915-53c42f01ccea?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "# These hyperparameters can be set by your requirements, the only hyperparameter that can't be changed is train_file \n",
    "hyperparameters = {\n",
    "    'train_file':'/opt/ml/input/data/train/{}'.format(training_file_name),\n",
    "    'num_train_epochs': 1,\n",
    "    'per_device_train_batch_size': 8,\n",
    "    'per_device_eval_batch_size': 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-20 15:54:55 Starting - Starting the training job...\n",
      "2023-01-20 15:55:20 Starting - Preparing the instances for trainingProfilerReport-1674230095: InProgress\n",
      "......\n",
      "2023-01-20 15:56:20 Downloading - Downloading input data...\n",
      "2023-01-20 15:56:40 Training - Downloading the training image...........................\n",
      "2023-01-20 16:01:21 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-01-20 16:01:30,822 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-01-20 16:01:30,840 sagemaker-training-toolkit INFO     Failed to parse hyperparameter model_name_or_path value Yvanzhu/Data-to-text-generation-accelerate to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2023-01-20 16:01:30,840 sagemaker-training-toolkit INFO     Failed to parse hyperparameter output_dir value /opt/ml/model to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2023-01-20 16:01:30,840 sagemaker-training-toolkit INFO     Failed to parse hyperparameter sagemaker_program value /opt/pytorch/run_summarization.py to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2023-01-20 16:01:30,840 sagemaker-training-toolkit INFO     Failed to parse hyperparameter train_file value /opt/ml/input/data/train/train_hotels_not_array.json to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2023-01-20 16:01:30,847 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-01-20 16:01:30,850 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-01-20 16:01:30,869 sagemaker-training-toolkit INFO     Failed to parse hyperparameter model_name_or_path value Yvanzhu/Data-to-text-generation-accelerate to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2023-01-20 16:01:30,869 sagemaker-training-toolkit INFO     Failed to parse hyperparameter output_dir value /opt/ml/model to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2023-01-20 16:01:30,869 sagemaker-training-toolkit INFO     Failed to parse hyperparameter sagemaker_program value /opt/pytorch/run_summarization.py to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2023-01-20 16:01:30,869 sagemaker-training-toolkit INFO     Failed to parse hyperparameter train_file value /opt/ml/input/data/train/train_hotels_not_array.json to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2023-01-20 16:01:30,895 sagemaker-training-toolkit INFO     Failed to parse hyperparameter model_name_or_path value Yvanzhu/Data-to-text-generation-accelerate to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2023-01-20 16:01:30,895 sagemaker-training-toolkit INFO     Failed to parse hyperparameter output_dir value /opt/ml/model to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2023-01-20 16:01:30,895 sagemaker-training-toolkit INFO     Failed to parse hyperparameter sagemaker_program value /opt/pytorch/run_summarization.py to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2023-01-20 16:01:30,895 sagemaker-training-toolkit INFO     Failed to parse hyperparameter train_file value /opt/ml/input/data/train/train_hotels_not_array.json to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2023-01-20 16:01:30,920 sagemaker-training-toolkit INFO     Failed to parse hyperparameter model_name_or_path value Yvanzhu/Data-to-text-generation-accelerate to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2023-01-20 16:01:30,920 sagemaker-training-toolkit INFO     Failed to parse hyperparameter output_dir value /opt/ml/model to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2023-01-20 16:01:30,920 sagemaker-training-toolkit INFO     Failed to parse hyperparameter sagemaker_program value /opt/pytorch/run_summarization.py to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2023-01-20 16:01:30,920 sagemaker-training-toolkit INFO     Failed to parse hyperparameter train_file value /opt/ml/input/data/train/train_hotels_not_array.json to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2023-01-20 16:01:30,927 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"do_train\": true,\n",
      "        \"model_name_or_path\": \"Yvanzhu/Data-to-text-generation-accelerate\",\n",
      "        \"num_train_epochs\": 1,\n",
      "        \"output_dir\": \"/opt/ml/model\",\n",
      "        \"per_device_eval_batch_size\": 8,\n",
      "        \"per_device_train_batch_size\": 8,\n",
      "        \"train_file\": \"/opt/ml/input/data/train/train_hotels_not_array.json\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"smart-descriptions-marketplace-2023-01-20-15-54-55-576\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/code\",\n",
      "    \"module_name\": \"/opt/pytorch/run_summarization\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"/opt/pytorch/run_summarization.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"do_train\":true,\"model_name_or_path\":\"Yvanzhu/Data-to-text-generation-accelerate\",\"num_train_epochs\":1,\"output_dir\":\"/opt/ml/model\",\"per_device_eval_batch_size\":8,\"per_device_train_batch_size\":8,\"train_file\":\"/opt/ml/input/data/train/train_hotels_not_array.json\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=/opt/pytorch/run_summarization.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=/opt/pytorch/run_summarization\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/code\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"do_train\":true,\"model_name_or_path\":\"Yvanzhu/Data-to-text-generation-accelerate\",\"num_train_epochs\":1,\"output_dir\":\"/opt/ml/model\",\"per_device_eval_batch_size\":8,\"per_device_train_batch_size\":8,\"train_file\":\"/opt/ml/input/data/train/train_hotels_not_array.json\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"smart-descriptions-marketplace-2023-01-20-15-54-55-576\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/code\",\"module_name\":\"/opt/pytorch/run_summarization\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"/opt/pytorch/run_summarization.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--do_train\",\"True\",\"--model_name_or_path\",\"Yvanzhu/Data-to-text-generation-accelerate\",\"--num_train_epochs\",\"1\",\"--output_dir\",\"/opt/ml/model\",\"--per_device_eval_batch_size\",\"8\",\"--per_device_train_batch_size\",\"8\",\"--train_file\",\"/opt/ml/input/data/train/train_hotels_not_array.json\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=Yvanzhu/Data-to-text-generation-accelerate\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_FILE=/opt/ml/input/data/train/train_hotels_not_array.json\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.22b20220929-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 /opt/pytorch/run_summarization.py --do_train True --model_name_or_path Yvanzhu/Data-to-text-generation-accelerate --num_train_epochs 1 --output_dir /opt/ml/model --per_device_eval_batch_size 8 --per_device_train_batch_size 8 --train_file /opt/ml/input/data/train/train_hotels_not_array.json\u001b[0m\n",
      "\u001b[34m01/20/2023 16:01:35 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m01/20/2023 16:01:35 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mbf16=False,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdo_eval=False,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=True,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=IntervalStrategy.NO,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mgeneration_max_length=None,\u001b[0m\n",
      "\u001b[34mgeneration_num_beams=None,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=1,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=False,\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_strategy=HubStrategy.EVERY_SAVE,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=5e-05,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=-1,\u001b[0m\n",
      "\u001b[34mlog_level=-1,\u001b[0m\n",
      "\u001b[34mlog_level_replica=-1,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=/opt/ml/model/runs/Jan20_16-01-35_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=500,\u001b[0m\n",
      "\u001b[34mlogging_strategy=IntervalStrategy.STEPS,\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=SchedulerType.LINEAR,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=1.0,\u001b[0m\n",
      "\u001b[34moptim=OptimizerNames.ADAMW_HF,\u001b[0m\n",
      "\u001b[34moutput_dir=/opt/ml/model,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=8,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=8,\u001b[0m\n",
      "\u001b[34mpredict_with_generate=False,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=/opt/ml/model,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=IntervalStrategy.STEPS,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=42,\u001b[0m\n",
      "\u001b[34msharded_ddp=[],\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34msortish_sampler=False,\u001b[0m\n",
      "\u001b[34mtf32=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0,\u001b[0m\n",
      "\u001b[34mxpu_backend=None,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m01/20/2023 16:01:35 - WARNING - datasets.builder - Using custom data configuration default-568b181c2781a105\u001b[0m\n",
      "\u001b[34m01/20/2023 16:01:35 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-568b181c2781a105/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-568b181c2781a105/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:00<00:00, 7013.89it/s]\u001b[0m\n",
      "\u001b[34m01/20/2023 16:01:35 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\u001b[0m\n",
      "\u001b[34m01/20/2023 16:01:35 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [00:00<00:00, 1241.65it/s]\u001b[0m\n",
      "\u001b[34m01/20/2023 16:01:35 - INFO - datasets.utils.info_utils - Unable to verify checksums.\u001b[0m\n",
      "\u001b[34m01/20/2023 16:01:35 - INFO - datasets.builder - Generating split train\u001b[0m\n",
      "\u001b[34m01/20/2023 16:01:35 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\u001b[0m\n",
      "\u001b[34mDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-568b181c2781a105/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]#015100%|██████████| 1/1 [00:00<00:00, 689.63it/s]\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:646] 2023-01-20 16:01:35,280 >> loading configuration file /opt/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:646] 2023-01-20 16:01:35,280 >> loading configuration file /opt/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:684] 2023-01-20 16:01:35,281 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"/opt/model\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:684] 2023-01-20 16:01:35,281 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"/opt/model\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1703] 2023-01-20 16:01:35,282 >> Didn't find file /opt/model/added_tokens.json. We won't load it.\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1784] 2023-01-20 16:01:35,282 >> loading file /opt/model/spiece.model\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1784] 2023-01-20 16:01:35,282 >> loading file /opt/model/tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1784] 2023-01-20 16:01:35,282 >> loading file None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1784] 2023-01-20 16:01:35,282 >> loading file /opt/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1784] 2023-01-20 16:01:35,282 >> loading file /opt/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1703] 2023-01-20 16:01:35,282 >> Didn't find file /opt/model/added_tokens.json. We won't load it.\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1784] 2023-01-20 16:01:35,282 >> loading file /opt/model/spiece.model\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1784] 2023-01-20 16:01:35,282 >> loading file /opt/model/tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1784] 2023-01-20 16:01:35,282 >> loading file None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1784] 2023-01-20 16:01:35,282 >> loading file /opt/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1784] 2023-01-20 16:01:35,282 >> loading file /opt/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:646] 2023-01-20 16:01:35,344 >> loading configuration file /opt/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:646] 2023-01-20 16:01:35,344 >> loading configuration file /opt/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:684] 2023-01-20 16:01:35,345 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"/opt/model\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:684] 2023-01-20 16:01:35,345 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"/opt/model\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1429] 2023-01-20 16:01:35,362 >> loading weights file /opt/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1429] 2023-01-20 16:01:35,362 >> loading weights file /opt/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1702] 2023-01-20 16:01:37,946 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1710] 2023-01-20 16:01:37,946 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at /opt/model.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1702] 2023-01-20 16:01:37,946 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1710] 2023-01-20 16:01:37,946 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at /opt/model.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\u001b[0m\n",
      "\u001b[34m01/20/2023 16:01:38 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.preprocess_function at 0x7f87657558b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m01/20/2023 16:01:38 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-568b181c2781a105/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-1c80317fa3b1799d.arrow\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 1/1 [00:00<00:00,  4.87ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset: 100%|██████████| 1/1 [00:00<00:00,  4.87ba/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1279] 2023-01-20 16:01:42,905 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1280] 2023-01-20 16:01:42,905 >>   Num examples = 918\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1279] 2023-01-20 16:01:42,905 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1280] 2023-01-20 16:01:42,905 >>   Num examples = 918\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1281] 2023-01-20 16:01:42,905 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1282] 2023-01-20 16:01:42,905 >>   Instantaneous batch size per device = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1283] 2023-01-20 16:01:42,905 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1284] 2023-01-20 16:01:42,905 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1281] 2023-01-20 16:01:42,905 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1282] 2023-01-20 16:01:42,905 >>   Instantaneous batch size per device = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1283] 2023-01-20 16:01:42,905 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1284] 2023-01-20 16:01:42,905 >>   Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1285] 2023-01-20 16:01:42,905 >>   Total optimization steps = 115\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1285] 2023-01-20 16:01:42,905 >>   Total optimization steps = 115\u001b[0m\n",
      "\u001b[34m0%|          | 0/115 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-01-20 16:01:43.085 algo-1:36 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-01-20 16:01:43.227 algo-1:36 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-01-20 16:01:43.229 algo-1:36 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-01-20 16:01:43.229 algo-1:36 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-01-20 16:01:43.230 algo-1:36 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-01-20 16:01:43.230 algo-1:36 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m1%|          | 1/115 [00:01<03:33,  1.87s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 2/115 [00:02<02:08,  1.13s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 3/115 [00:02<01:30,  1.24it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 4/115 [00:03<01:11,  1.55it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 5/115 [00:03<01:01,  1.80it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 6/115 [00:04<01:01,  1.79it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 7/115 [00:04<01:02,  1.73it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 8/115 [00:05<01:02,  1.70it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 9/115 [00:05<00:58,  1.81it/s]\u001b[0m\n",
      "\u001b[34m9%|▊         | 10/115 [00:06<00:54,  1.92it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 11/115 [00:06<00:55,  1.88it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 12/115 [00:07<00:55,  1.86it/s]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 13/115 [00:07<00:52,  1.95it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 14/115 [00:08<00:53,  1.88it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 15/115 [00:09<00:52,  1.91it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 16/115 [00:09<00:52,  1.90it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 17/115 [00:10<00:53,  1.84it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 18/115 [00:10<00:55,  1.75it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 19/115 [00:11<00:53,  1.80it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 20/115 [00:11<00:53,  1.78it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 21/115 [00:12<00:49,  1.89it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 22/115 [00:12<00:49,  1.87it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 23/115 [00:13<00:52,  1.76it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 24/115 [00:14<00:49,  1.83it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 25/115 [00:14<00:47,  1.90it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 26/115 [00:15<00:45,  1.96it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 27/115 [00:15<00:46,  1.88it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 28/115 [00:16<00:47,  1.83it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 29/115 [00:16<00:44,  1.91it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 30/115 [00:17<00:44,  1.89it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 31/115 [00:17<00:43,  1.92it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 32/115 [00:18<00:42,  1.97it/s]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 33/115 [00:18<00:43,  1.90it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 34/115 [00:19<00:44,  1.82it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 35/115 [00:19<00:44,  1.81it/s]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 36/115 [00:20<00:43,  1.82it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 37/115 [00:20<00:43,  1.81it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 38/115 [00:21<00:43,  1.78it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 39/115 [00:22<00:44,  1.72it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 40/115 [00:22<00:44,  1.68it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 41/115 [00:23<00:41,  1.78it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 42/115 [00:23<00:40,  1.82it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 43/115 [00:24<00:39,  1.80it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 44/115 [00:24<00:38,  1.82it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 45/115 [00:25<00:39,  1.79it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 46/115 [00:26<00:38,  1.81it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 47/115 [00:26<00:36,  1.85it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 48/115 [00:27<00:35,  1.88it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 49/115 [00:27<00:36,  1.78it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 50/115 [00:28<00:37,  1.75it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 51/115 [00:28<00:36,  1.76it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 52/115 [00:29<00:35,  1.79it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 53/115 [00:30<00:36,  1.71it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 54/115 [00:30<00:33,  1.84it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 55/115 [00:31<00:32,  1.84it/s]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 56/115 [00:31<00:32,  1.79it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 57/115 [00:32<00:34,  1.71it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 58/115 [00:32<00:32,  1.74it/s]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 59/115 [00:33<00:31,  1.79it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 60/115 [00:33<00:30,  1.82it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 61/115 [00:34<00:29,  1.85it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 62/115 [00:34<00:27,  1.93it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 63/115 [00:35<00:28,  1.81it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 64/115 [00:36<00:29,  1.71it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 65/115 [00:36<00:28,  1.73it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 66/115 [00:37<00:30,  1.63it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 67/115 [00:38<00:29,  1.62it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 68/115 [00:38<00:27,  1.69it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 69/115 [00:39<00:27,  1.69it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 70/115 [00:39<00:27,  1.66it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 71/115 [00:40<00:24,  1.82it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 72/115 [00:40<00:24,  1.76it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 73/115 [00:41<00:22,  1.83it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 74/115 [00:41<00:22,  1.83it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 75/115 [00:42<00:22,  1.81it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 76/115 [00:42<00:21,  1.82it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 77/115 [00:43<00:21,  1.76it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 78/115 [00:44<00:20,  1.82it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 79/115 [00:44<00:20,  1.79it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 80/115 [00:45<00:19,  1.84it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 81/115 [00:45<00:18,  1.82it/s]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 82/115 [00:46<00:18,  1.80it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 83/115 [00:46<00:18,  1.77it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 84/115 [00:47<00:17,  1.78it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 85/115 [00:48<00:17,  1.76it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 86/115 [00:48<00:15,  1.89it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 87/115 [00:48<00:14,  1.92it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 88/115 [00:49<00:14,  1.86it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 89/115 [00:49<00:13,  1.98it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 90/115 [00:50<00:13,  1.92it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 91/115 [00:51<00:12,  1.98it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 92/115 [00:51<00:12,  1.88it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 93/115 [00:52<00:11,  1.86it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 94/115 [00:52<00:10,  1.94it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 95/115 [00:53<00:10,  1.99it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 96/115 [00:53<00:10,  1.82it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 97/115 [00:54<00:09,  1.81it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 98/115 [00:54<00:09,  1.87it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 99/115 [00:55<00:08,  1.90it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 100/115 [00:55<00:08,  1.86it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 101/115 [00:56<00:07,  1.89it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 102/115 [00:56<00:07,  1.85it/s]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 103/115 [00:57<00:06,  1.99it/s]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 104/115 [00:57<00:05,  1.96it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 105/115 [00:58<00:05,  1.94it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 106/115 [00:59<00:04,  1.84it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 107/115 [00:59<00:04,  1.97it/s]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 108/115 [01:00<00:03,  1.84it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 109/115 [01:00<00:03,  1.77it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 110/115 [01:01<00:02,  1.77it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 111/115 [01:01<00:02,  1.72it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 112/115 [01:02<00:01,  1.88it/s]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 113/115 [01:02<00:01,  1.90it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 114/115 [01:03<00:00,  1.95it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 115/115 [01:03<00:00,  2.13it/s]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1508] 2023-01-20 16:02:46,583 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1508] 2023-01-20 16:02:46,583 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m{'train_runtime': 63.7019, 'train_samples_per_second': 14.411, 'train_steps_per_second': 1.805, 'train_loss': 2.113275677224864, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 115/115 [01:03<00:00,  2.13it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 115/115 [01:03<00:00,  1.81it/s]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2139] 2023-01-20 16:02:46,608 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2139] 2023-01-20 16:02:46,608 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:439] 2023-01-20 16:02:46,609 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:439] 2023-01-20 16:02:46,609 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\n",
      "2023-01-20 16:03:02 Uploading - Uploading generated training model\u001b[34m[INFO|modeling_utils.py:1084] 2023-01-20 16:02:47,992 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1084] 2023-01-20 16:02:47,992 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2094] 2023-01-20 16:02:47,993 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2094] 2023-01-20 16:02:47,993 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2100] 2023-01-20 16:02:47,993 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2100] 2023-01-20 16:02:47,993 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_t5_fast.py:162] 2023-01-20 16:02:48,034 >> Copy vocab file to /opt/ml/model/spiece.model\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_t5_fast.py:162] 2023-01-20 16:02:48,034 >> Copy vocab file to /opt/ml/model/spiece.model\u001b[0m\n",
      "\u001b[34m***** train metrics *****\n",
      "  epoch                    =        1.0\u001b[0m\n",
      "\u001b[34mtrain_loss               =     2.1133\n",
      "  train_runtime            = 0:01:03.70\n",
      "  train_samples            =        918\n",
      "  train_samples_per_second =     14.411\n",
      "  train_steps_per_second   =      1.805\u001b[0m\n",
      "\u001b[34m2023-01-20 16:02:48,808 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-01-20 16:02:48,808 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-01-20 16:02:48,809 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-01-20 16:05:05 Completed - Training job completed\n",
      "Training seconds: 539\n",
      "Billable seconds: 539\n"
     ]
    }
   ],
   "source": [
    "# Create an estimator object for running a training job\n",
    "\n",
    "instance_type= '<InstanceType>' # Replace with your instance type. Supported instances types: ml.m5.large, ml.m5.xlarge, ml.m5.2xlarge, ml.g4dn.xlarge, ml.g4dn.2xlarge\n",
    "\n",
    "estimator = sage.algorithm.AlgorithmEstimator(\n",
    "    algorithm_arn=algo_arn,\n",
    "    base_job_name=\"smart-descriptions-marketplace\",\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    input_mode=\"File\",\n",
    "    output_path=output_location,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters=hyperparameters,\n",
    ")\n",
    "# Run the training job.\n",
    "estimator.fit({\"train\": training_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Deploy model and verify results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can deploy the model for performing real-time inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"smart-descriptions\"\n",
    "\n",
    "content_type = \"application/json\"\n",
    "\n",
    "real_time_inference_instance_type = \"<InstanceType>\" # Replace with your instance type. Supported instances types: ml.m5.large, ml.m5.xlarge, ml.m5.2xlarge\n",
    "\n",
    "batch_transform_inference_instance_type = \"<InstanceType>\" # Replace with your instance type. Supported instances types: ml.m5.large, ml.m5.xlarge, ml.m5.2xlarge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Deploy trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........\n",
      "-----------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    1, real_time_inference_instance_type, serializer=sage.serializers.JSONSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once endpoint is created, you can perform real-time inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Create input payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'input-real-time-inference.txt'\n",
    "input_data = './data/inference/input/real-time/{}'.format(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_endpoint = []\n",
    "with open(input_data) as f:\n",
    "    for line in f:\n",
    "        input_data_endpoint.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<name> name = [ 1 Hotel South Beach ] ; <address> address = [ 2341 Collins Ave, Miami Beach, FL 33139, USA ] ; <feelsHotel> feelsHotel = [ luxury ] ; <hasBarOnsite> hasBarOnsite = [ yes ] ; <hasDeskInRooms> hasDeskInRooms = [ yes ] ; <hasBalconyInRooms> hasBalconyInRooms = [ yes ] ; <hasRoomsUpgraded> hasRoomsUpgraded = [ yes ] ; <hasKitchenInRoom> hasKitchenInRoom = [ yes ]',\n",
       " '<name> name = [ Ala Moana Hotel ] ; <address> address = [ 410 Atkinson Drive, Honolulu, HI 96814, USA ] ; <feelsHotel> feelsHotel = [ casual ] ; <hasConventionCenter> hasConventionCenter = [ yes ] ; <hasOnsiteCafe> hasOnsiteCafe = [ yes ] ; <hasCoffeeInRooms> hasCoffeeInRooms = [ yes ] ; <hasMinifridgeInRooms> hasMinifridgeInRooms = [ yes ] ; <hasBalconyInRooms> hasBalconyInRooms = [ yes ] ; <hasMicrowaveInRooms> hasMicrowaveInRooms = [ yes ]',\n",
       " '<name> name = [ Belvedere Hotel ] ; <address> address = [ 1900 Boardwalk, Atlantic City, NJ 08401, USA ] ; <hasBarOnsite> hasBarOnsite = [ yes ] ; <hasRestaurant> hasRestaurant = [ yes ] ; <hasMeetingRooms> hasMeetingRooms = [ yes ] ; <hasSpa> hasSpa = [ yes ] ; <hasCasino> hasCasino = [ yes ]']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data_endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters definition to implement a prediction:\n",
    "* max_length (int): The maximum length of the sequence to be generated.\n",
    "* min_length (int): The minimum length of the sequence to be generated\n",
    "* length_penalty (float, optional, defaults to 1.0): Exponential penalty to the length that is used with beam-based generation. It is applied as an exponent to the sequence length, which in turn is used to divide the score of the sequence. Since the score is the log likelihood of the sequence (i.e. negative), length_penalty > 0.0 promotes longer sequences, while length_penalty < 0.0 encourages shorter sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_parameters = {\n",
    "\t\t\"max_length\": 150,\n",
    "\t\t\"min_length\": 30,\n",
    "\t\t\"length_penalty\": 3.0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Add code snippet that shows the payload contents>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Perform real-time inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'[{\"generated_text\":\"The 1 Hotel South Beach is a luxury hotel. It has a bar and a balcony. It has upgraded rooms and a kitchen.\"},{\"generated_text\":\"The Ala Moana Hotel is a casual hotel that offers a conference center and a coffee maker. It has a balcony and a mini fridge.\"},{\"generated_text\":\"The Belvedere Hotel is a great choice for you. It has a bar, a restaurant, a meeting room, a spa and a casino.\"}]'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = predictor.predict({\n",
    "\t'inputs': input_data_endpoint,\n",
    "\t'parameters': prediction_parameters\n",
    "})\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The 1 Hotel South Beach is a luxury hotel. It has a bar and a balcony. It has upgraded rooms and a kitchen.'},\n",
       " {'generated_text': 'The Ala Moana Hotel is a casual hotel that offers a conference center and a coffee maker. It has a balcony and a mini fridge.'},\n",
       " {'generated_text': 'The Belvedere Hotel is a great choice for you. It has a bar, a restaurant, a meeting room, a spa and a casino.'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = json.loads(prediction)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/inference/output/real-time/output.txt', 'w') as outfile:\n",
    "    for entry in output:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Delete the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully performed a real-time inference, you do not need the endpoint any more. you can terminate the same to avoid being charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Perform Batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will perform batch inference using multiple input payloads together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform input uploaded to s3://sagemaker-us-east-1-544022947556/smart-descriptions/batch/input\n"
     ]
    }
   ],
   "source": [
    "# upload the batch-transform job input files to S3\n",
    "transform_input_folder = \"data/inference/input/batch\" \n",
    "transform_input = sagemaker_session.upload_data(transform_input_folder, key_prefix=model_name + '/batch/input')\n",
    "print(\"Transform input uploaded to \" + transform_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........\n",
      "...................................................\u001b[34mWarning: MMS is using non-default JVM parameters: -XX:-UseContainerSupport\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,677 [INFO ] main com.amazonaws.ml.mms.ModelServer - \u001b[0m\n",
      "\u001b[34mMMS Home: /opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mCurrent directory: /\u001b[0m\n",
      "\u001b[34mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[34mNumber of GPUs: 0\u001b[0m\n",
      "\u001b[34mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[34mMax heap size: 3499 M\u001b[0m\n",
      "\u001b[34mPython executable: /opt/conda/bin/python3.8\u001b[0m\n",
      "\u001b[34mConfig file: /etc/sagemaker-mms.properties\u001b[0m\n",
      "\u001b[34mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mModel Store: /.sagemaker/mms/models\u001b[0m\n",
      "\u001b[34mInitial Models: ALL\u001b[0m\n",
      "\u001b[34mLog dir: null\u001b[0m\n",
      "\u001b[34mMetrics dir: null\u001b[0m\n",
      "\u001b[34mNetty threads: 0\u001b[0m\n",
      "\u001b[34mNetty client threads: 0\u001b[0m\n",
      "\u001b[34mDefault workers per model: 4\u001b[0m\n",
      "\u001b[34mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[34mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[34mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[34mPreload model: false\u001b[0m\n",
      "\u001b[34mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,733 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-9000-model\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,794 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - model_service_worker started with args: --sock-type unix --sock-name /home/model-server/tmp/.mms.sock.9000 --handler sagemaker_huggingface_inference_toolkit.handler_service --model-path /.sagemaker/mms/models/model --model-name model --preload-model false --tmp-dir /home/model-server/tmp\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,795 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,795 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID] 40\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,795 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MMS worker started.\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,796 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.8.10\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,796 [INFO ] main com.amazonaws.ml.mms.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,804 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,817 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,817 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,817 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,819 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,886 [INFO ] main com.amazonaws.ml.mms.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mModel server started.\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,889 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,891 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,901 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,901 [WARN ] pool-3-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,907 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:15,171 [INFO ] pool-2-thread-6 ACCESS_LOG - /169.254.255.130:44832 \"GET /ping HTTP/1.1\" 200 27\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:15,893 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:44846 \"GET /execution-parameters HTTP/1.1\" 404 2\u001b[0m\n",
      "\u001b[32m2023-01-20T16:38:15.914:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=SINGLE_RECORD\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:22,458 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000016-00000000-7b27e130060b8829-e16e8e49\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:22,462 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 7433\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:22,465 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-2\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:22,458 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000016-00000000-7b27e130060b8829-e16e8e49\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:22,462 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 7433\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:22,465 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-2\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:22,747 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000016-00000002-2eb81130060b8829-9816ebac\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:22,748 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 7713\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:22,749 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-1\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:22,796 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000016-00000003-884c1130060b8829-d54df4b0\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:22,798 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 7763\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:22,799 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-3\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:22,905 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000016-00000004-d92f9130060b882a-cbf4f89c\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:22,907 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 7872\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:22,907 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-4\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:22,747 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000016-00000002-2eb81130060b8829-9816ebac\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:22,748 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 7713\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:22,749 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-1\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:22,796 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000016-00000003-884c1130060b8829-d54df4b0\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:22,798 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 7763\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:22,799 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-3\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:22,905 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000016-00000004-d92f9130060b882a-cbf4f89c\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:22,907 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 7872\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:22,907 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-4\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:24,831 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.04553794860839844 ms\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:24,831 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 2354\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:24,831 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 2352.74338722229 ms\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:24,831 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.0514984130859375 ms\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:24,831 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.04553794860839844 ms\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:24,831 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 2354\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:24,831 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 2352.74338722229 ms\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:24,831 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.0514984130859375 ms\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:24,832 [INFO ] W-9000-model ACCESS_LOG - /169.254.255.130:44854 \"POST /invocations HTTP/1.1\" 200 8804\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:24,832 [INFO ] W-9000-model ACCESS_LOG - /169.254.255.130:44854 \"POST /invocations HTTP/1.1\" 200 8804\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:27,450 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.03147125244140625 ms\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:27,451 [INFO ] W-9000-model ACCESS_LOG - /169.254.255.130:44854 \"POST /invocations HTTP/1.1\" 200 2563\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:27,451 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 2560.8954429626465 ms\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:27,452 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.0457763671875 ms\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:27,450 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.03147125244140625 ms\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:27,451 [INFO ] W-9000-model ACCESS_LOG - /169.254.255.130:44854 \"POST /invocations HTTP/1.1\" 200 2563\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:27,451 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 2560.8954429626465 ms\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:27,452 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.0457763671875 ms\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:29,882 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 2420\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:29,883 [INFO ] W-9000-model ACCESS_LOG - /169.254.255.130:44854 \"POST /invocations HTTP/1.1\" 200 2422\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:29,882 [INFO ] W-model-3-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.041484832763671875 ms\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:29,884 [INFO ] W-model-3-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 2419.6224212646484 ms\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:29,884 [INFO ] W-model-3-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.044345855712890625 ms\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:29,882 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 2420\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:29,883 [INFO ] W-9000-model ACCESS_LOG - /169.254.255.130:44854 \"POST /invocations HTTP/1.1\" 200 2422\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:29,882 [INFO ] W-model-3-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.041484832763671875 ms\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:29,884 [INFO ] W-model-3-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 2419.6224212646484 ms\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:29,884 [INFO ] W-model-3-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.044345855712890625 ms\u001b[0m\n",
      "\n",
      "\u001b[34mWarning: MMS is using non-default JVM parameters: -XX:-UseContainerSupport\u001b[0m\n",
      "\u001b[35mWarning: MMS is using non-default JVM parameters: -XX:-UseContainerSupport\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,677 [INFO ] main com.amazonaws.ml.mms.ModelServer - \u001b[0m\n",
      "\u001b[34mMMS Home: /opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mCurrent directory: /\u001b[0m\n",
      "\u001b[34mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[34mNumber of GPUs: 0\u001b[0m\n",
      "\u001b[34mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[34mMax heap size: 3499 M\u001b[0m\n",
      "\u001b[34mPython executable: /opt/conda/bin/python3.8\u001b[0m\n",
      "\u001b[34mConfig file: /etc/sagemaker-mms.properties\u001b[0m\n",
      "\u001b[34mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mModel Store: /.sagemaker/mms/models\u001b[0m\n",
      "\u001b[34mInitial Models: ALL\u001b[0m\n",
      "\u001b[34mLog dir: null\u001b[0m\n",
      "\u001b[34mMetrics dir: null\u001b[0m\n",
      "\u001b[34mNetty threads: 0\u001b[0m\n",
      "\u001b[34mNetty client threads: 0\u001b[0m\n",
      "\u001b[34mDefault workers per model: 4\u001b[0m\n",
      "\u001b[34mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[34mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[34mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[34mPreload model: false\u001b[0m\n",
      "\u001b[34mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,733 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-9000-model\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,794 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - model_service_worker started with args: --sock-type unix --sock-name /home/model-server/tmp/.mms.sock.9000 --handler sagemaker_huggingface_inference_toolkit.handler_service --model-path /.sagemaker/mms/models/model --model-name model --preload-model false --tmp-dir /home/model-server/tmp\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:14,677 [INFO ] main com.amazonaws.ml.mms.ModelServer - \u001b[0m\n",
      "\u001b[35mMMS Home: /opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[35mCurrent directory: /\u001b[0m\n",
      "\u001b[35mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[35mNumber of GPUs: 0\u001b[0m\n",
      "\u001b[35mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[35mMax heap size: 3499 M\u001b[0m\n",
      "\u001b[35mPython executable: /opt/conda/bin/python3.8\u001b[0m\n",
      "\u001b[35mConfig file: /etc/sagemaker-mms.properties\u001b[0m\n",
      "\u001b[35mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[35mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[35mModel Store: /.sagemaker/mms/models\u001b[0m\n",
      "\u001b[35mInitial Models: ALL\u001b[0m\n",
      "\u001b[35mLog dir: null\u001b[0m\n",
      "\u001b[35mMetrics dir: null\u001b[0m\n",
      "\u001b[35mNetty threads: 0\u001b[0m\n",
      "\u001b[35mNetty client threads: 0\u001b[0m\n",
      "\u001b[35mDefault workers per model: 4\u001b[0m\n",
      "\u001b[35mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[35mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[35mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[35mPreload model: false\u001b[0m\n",
      "\u001b[35mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:14,733 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-9000-model\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:14,794 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - model_service_worker started with args: --sock-type unix --sock-name /home/model-server/tmp/.mms.sock.9000 --handler sagemaker_huggingface_inference_toolkit.handler_service --model-path /.sagemaker/mms/models/model --model-name model --preload-model false --tmp-dir /home/model-server/tmp\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,795 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,795 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID] 40\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,795 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MMS worker started.\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,796 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.8.10\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,796 [INFO ] main com.amazonaws.ml.mms.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,804 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,817 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,817 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,817 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,819 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,886 [INFO ] main com.amazonaws.ml.mms.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mModel server started.\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,889 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,891 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,901 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,901 [WARN ] pool-3-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:14,907 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:15,171 [INFO ] pool-2-thread-6 ACCESS_LOG - /169.254.255.130:44832 \"GET /ping HTTP/1.1\" 200 27\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:14,795 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:14,795 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID] 40\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:14,795 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MMS worker started.\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:14,796 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.8.10\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:14,796 [INFO ] main com.amazonaws.ml.mms.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:14,804 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:14,817 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:14,817 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:14,817 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:14,819 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:14,886 [INFO ] main com.amazonaws.ml.mms.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[35mModel server started.\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:14,889 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:14,891 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:14,901 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:14,901 [WARN ] pool-3-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:14,907 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:15,171 [INFO ] pool-2-thread-6 ACCESS_LOG - /169.254.255.130:44832 \"GET /ping HTTP/1.1\" 200 27\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:15,893 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:44846 \"GET /execution-parameters HTTP/1.1\" 404 2\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:15,893 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:44846 \"GET /execution-parameters HTTP/1.1\" 404 2\u001b[0m\n",
      "\u001b[32m2023-01-20T16:38:15.914:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=SINGLE_RECORD\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:22,458 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000016-00000000-7b27e130060b8829-e16e8e49\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:22,462 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 7433\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:22,465 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-2\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:22,458 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000016-00000000-7b27e130060b8829-e16e8e49\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:22,462 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 7433\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:22,465 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-2\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:22,747 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000016-00000002-2eb81130060b8829-9816ebac\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:22,748 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 7713\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:22,749 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-1\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:22,796 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000016-00000003-884c1130060b8829-d54df4b0\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:22,798 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 7763\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:22,799 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-3\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:22,905 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000016-00000004-d92f9130060b882a-cbf4f89c\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:22,907 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 7872\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:22,907 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-4\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:22,747 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000016-00000002-2eb81130060b8829-9816ebac\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:22,748 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 7713\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:22,749 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-1\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:22,796 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000016-00000003-884c1130060b8829-d54df4b0\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:22,798 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 7763\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:22,799 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-3\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:22,905 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000016-00000004-d92f9130060b882a-cbf4f89c\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:22,907 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 7872\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:22,907 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-4\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:24,831 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.04553794860839844 ms\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:24,831 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 2354\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:24,831 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 2352.74338722229 ms\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:24,831 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.0514984130859375 ms\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:24,831 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.04553794860839844 ms\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:24,831 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 2354\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:24,831 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 2352.74338722229 ms\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:24,831 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.0514984130859375 ms\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:24,832 [INFO ] W-9000-model ACCESS_LOG - /169.254.255.130:44854 \"POST /invocations HTTP/1.1\" 200 8804\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:24,832 [INFO ] W-9000-model ACCESS_LOG - /169.254.255.130:44854 \"POST /invocations HTTP/1.1\" 200 8804\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:27,451 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 2561\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:27,450 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.03147125244140625 ms\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:27,451 [INFO ] W-9000-model ACCESS_LOG - /169.254.255.130:44854 \"POST /invocations HTTP/1.1\" 200 2563\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:27,451 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 2560.8954429626465 ms\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:27,452 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.0457763671875 ms\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:27,451 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 2561\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:27,450 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.03147125244140625 ms\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:27,451 [INFO ] W-9000-model ACCESS_LOG - /169.254.255.130:44854 \"POST /invocations HTTP/1.1\" 200 2563\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:27,451 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 2560.8954429626465 ms\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:27,452 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.0457763671875 ms\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:29,882 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 2420\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:29,883 [INFO ] W-9000-model ACCESS_LOG - /169.254.255.130:44854 \"POST /invocations HTTP/1.1\" 200 2422\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:29,882 [INFO ] W-model-3-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.041484832763671875 ms\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:29,884 [INFO ] W-model-3-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 2419.6224212646484 ms\u001b[0m\n",
      "\u001b[34m2023-01-20T16:38:29,884 [INFO ] W-model-3-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.044345855712890625 ms\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:29,882 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 2420\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:29,883 [INFO ] W-9000-model ACCESS_LOG - /169.254.255.130:44854 \"POST /invocations HTTP/1.1\" 200 2422\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:29,882 [INFO ] W-model-3-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.041484832763671875 ms\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:29,884 [INFO ] W-model-3-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 2419.6224212646484 ms\u001b[0m\n",
      "\u001b[35m2023-01-20T16:38:29,884 [INFO ] W-model-3-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.044345855712890625 ms\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run the batch-transform job\n",
    "transformer = estimator.transformer(1, batch_transform_inference_instance_type, strategy='SingleRecord', \n",
    "                                    output_path= 's3://{}/{}/batch/output/'.format(bucket.name, model_name),\n",
    "                                   assemble_with='Line')\n",
    "transformer.transform(transform_input, content_type=content_type, split_type='Line')\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output is available on following path\n",
    "transformer.output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterates through all the objects, doing the pagination for you. Each obj\n",
    "# is an ObjectSummary, so it doesn't contain the body. You'll need to call\n",
    "# get to get the whole body.\n",
    "obj = bucket.Object('{}/batch/output/input-batch-job.txt.out'.format(model_name))\n",
    "key = obj.key\n",
    "body = obj.get()['Body'].read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'[{\"generated_text\":\"The 1 Hotel South Beach is a luxury hotel. It has a bar and a balcony. It has upgraded rooms and a kitchen.\"}]\\n[{\"generated_text\":\"The Ala Moana Hotel is a casual hotel that offers a conference center and a coffee maker. It has a balcony and a mini fridge.\"}]\\n[{\"generated_text\":\"Belvedere Hote is a great choice for you. It has a bar, a restaurant, a meeting room, a spa and a casino.\"}]\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"generated_text\":\"The 1 Hotel South Beach is a luxury hotel. It has a bar and a balcony. It has upgraded rooms and a kitchen.\"},{\"generated_text\":\"The Ala Moana Hotel is a casual hotel that offers a conference center and a coffee maker. It has a balcony and a mini fridge.\"},{\"generated_text\":\"Belvedere Hote is a great choice for you. It has a bar, a restaurant, a meeting room, a spa and a casino.\"}]'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body = body.decode(\"utf-8\")\n",
    "body = body.replace(']\\n[', ',')\n",
    "body = body.replace(']\\n', ']')\n",
    "body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The 1 Hotel South Beach is a luxury hotel. It has a bar and a balcony. It has upgraded rooms and a kitchen.'},\n",
       " {'generated_text': 'The Ala Moana Hotel is a casual hotel that offers a conference center and a coffee maker. It has a balcony and a mini fridge.'},\n",
       " {'generated_text': 'Belvedere Hote is a great choice for you. It has a bar, a restaurant, a meeting room, a spa and a casino.'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = json.loads(body)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/inference/output/batch/output.txt', 'w') as outfile:\n",
    "    for entry in output:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.delete_model()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
